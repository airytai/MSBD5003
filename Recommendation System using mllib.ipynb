{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import *\n",
    "import random\n",
    "from operator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "userItemFile = sc.textFile(\"tianchi_fresh_comp_train_user.txt\")\n",
    "itemFile = sc.textFile(\"tianchi_fresh_comp_train_item.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'10001082\\t285259775\\t1', u'10001082\\t4368907\\t1', u'10001082\\t4368907\\t1', u'10001082\\t53616768\\t1', u'10001082\\t151466952\\t1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userItemFile.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'100002303\\t3368', u'100003592\\t7995', u'100006838\\t12630', u'100008089\\t7791', u'100012750\\t9614']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemFile.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser1(s):\n",
    "    temp = s.split(\"\\t\")\n",
    "    return (int(temp[0]),int(temp[1]),int(temp[2]))\n",
    "\n",
    "def parser2(s):\n",
    "    temp = s.split(\"\\t\")\n",
    "    return (int(temp[0]),int(temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "userItem = userItemFile.map(parser1).map(lambda x: ((x[0],x[1]),x[2])).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0][0],x[0][1],x[1]))\n",
    "item = itemFile.map(parser2).map(lambda x: (x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10238478, 83906594, 1), (107386463, 192914757, 2), (104448961, 297013625, 3), (101982646, 173753750, 1), (111435559, 334399291, 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userItem.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397276"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userItem.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100002303, 100002303), (100003592, 100003592), (100006838, 100006838), (100008089, 100008089), (100012750, 100012750)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620918"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, b = userItem.randomSplit([1,7], 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29691\n",
      "9846\n",
      "10050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData,validationData,testData = data.randomSplit([6,2,2], 13)\n",
    "# print(trainData.take(3))\n",
    "# print(validationData.take(3))\n",
    "# print(testData.take(3))\n",
    "print(trainData.count())\n",
    "print(validationData.count())\n",
    "print(testData.count())\n",
    "trainData.cache()\n",
    "validationData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEval(bestModel,validation):\n",
    "    import operator\n",
    "    userIds = validation.map(lambda x: x[0]).distinct().collect()\n",
    "    itemIds = list(item.collectAsMap().keys())\n",
    "    temp = trainData.map(lambda x: (x[0], x[1])).filter(lambda x: x[0] in userIds).groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "    print '1'\n",
    "    valiSet = temp.flatMap(lambda x: [(x[0],a) for a in itemIds if a not in x[1]])\n",
    "    print '2'\n",
    "    predictions = bestModel.predictAll(valiSet).map(lambda x: (x[0], x[1], x[2])).groupBy(lambda x:x[0]).map(lambda x:(x[0],sorted(list(x[1]),key=operator.itemgetter(2),reverse=True)))\n",
    "    print '2.5'\n",
    "    predictions = predictions.map(lambda x:(x[0],list(elmt[1] for elmt in x[1])))\n",
    "    print '3'\n",
    "    cmprSet = validation.groupBy(lambda x:x[0]).map(lambda x:(x[0],list(x[1]))).map(lambda x:(x[0],list(elmt[1] for elmt in x[1])))\n",
    "    print '4'\n",
    "    p = predictions.collect()\n",
    "    cdic =  cmprSet.collectAsMap()  \n",
    "    scores = []\n",
    "    print '5'\n",
    "    for i in range(0,len(p)):\n",
    "        user = p[i][0]\n",
    "        plist = p[i][1]\n",
    "        clist = cdic[user]\n",
    "        plen = len(plist)\n",
    "        clen = len(clist)\n",
    "        count = 0\n",
    "        score = 0\n",
    "        if plen >= clen:\n",
    "            for j in range(0,clen):\n",
    "                if plist[j] in clist:\n",
    "                    count += 1\n",
    "        score = float(float(count)/clen)\n",
    "        scores.append(score)\n",
    "    print '6'\n",
    "    return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "2\n",
      "2.5\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-1509ca97-63bb-4992-894a-a82ddae122c7/pyspark_runner.py\", line 189, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 7, in <module>\n",
       "  File \"<string>\", line 15, in modelEval\n",
       "  File \"/home/msbd5003/spark/python/pyspark/rdd.py\", line 809, in collect\n",
       "    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
       "  File \"/home/msbd5003/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
       "    answer = self.gateway_client.send_command(command)\n",
       "  File \"/home/msbd5003/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n",
       "    response = connection.send_command(command)\n",
       "  File \"/home/msbd5003/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n",
       "    answer = smart_decode(self.stream.readline()[:-1])\n",
       "  File \"/usr/lib/python2.7/socket.py\", line 451, in readline\n",
       "    data = self._sock.recv(self._rbufsize)\n",
       "  File \"/home/msbd5003/spark/python/pyspark/context.py\", line 237, in signal_handler\n",
       "    raise KeyboardInterrupt()\n",
       "KeyboardInterrupt\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "score = 0\n",
    "best_rank = 2\n",
    "for elmt in range(2,11,2):\n",
    "    print elmt\n",
    "    bestModel = ALS.trainImplicit(trainData, rank=elmt)\n",
    "    best_model_score = modelEval(bestModel,validationData)\n",
    "    if model_score > score:\n",
    "        score = best_model_score\n",
    "        best_rank = elmt\n",
    "\n",
    "print('The best model score for rank ' + str(best_rank) + ' is '+ str(best_model_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = ALS.trainImplicit(trainData, rank=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = bestModel.recommendProducts(104448961, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1: 237160017\n",
      "Item 2: 342449091\n",
      "Item 3: 335920490\n",
      "Item 4: 140342917\n",
      "Item 5: 44796654\n"
     ]
    }
   ],
   "source": [
    "lists = []\n",
    "for i in range(0,len(temp)):\n",
    "    lists.append(temp[i][1])\n",
    "    print(\"Item \"+str(i+1)+\": \"+str(temp[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ALS(train_data, validation_data, num_iters, reg_param, ranks):\n",
    "    \"\"\"\n",
    "    Grid Search Function to select the best model based on RMSE of hold-out data\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in reg_param:\n",
    "            # train ALS model\n",
    "            model = ALS.train(\n",
    "                ratings=train_data,    # (userID, productID, rating) tuple\n",
    "                iterations=num_iters,\n",
    "                rank=rank,\n",
    "                lambda_=reg,           # regularization param\n",
    "                seed=99)\n",
    "            # make prediction\n",
    "            valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
    "            predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "            # get the rating result\n",
    "            ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "            # get the RMSE\n",
    "            MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "            error = math.sqrt(MSE)\n",
    "            print('{} latent factors: validation RMSE is {}'.format(rank, error))\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors'.format(best_rank))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-param config\n",
    "num_iterations = 5\n",
    "ranks = [2, 4, 6, 8, 10]\n",
    "reg_params = [0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 latent factors: validation RMSE is 5.64686708211\n",
      "4 latent factors: validation RMSE is 5.64575302802\n",
      "6 latent factors: validation RMSE is 4.91813720378\n",
      "8 latent factors: validation RMSE is 4.94902796925\n",
      "10 latent factors: validation RMSE is 4.91260713016\n",
      "\n",
      "The best model has 10 latent factors\n"
     ]
    }
   ],
   "source": [
    "final_model = train_ALS(trainData, validationData, num_iterations, reg_params, ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.12\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
